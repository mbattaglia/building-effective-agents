{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directly calling LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will implement a simple approach to using LLMs by directly calling them.\n",
    "\n",
    "We will use LangChain to abstract away the LLM integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by installing some dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the content from the pdf files, we will define a custom function named (`read_files`). We will also define a custom class (`FileContent`) that we will use to store, as the name implies, content from files.\n",
    "\n",
    "We will use Pydantic for the class definition, as it comes with built-in validation, automatic data conversion and more. Our custom class will be very simple and only have 2 attributes:\n",
    "- `path`: to store the full path to the file\n",
    "- `content`: the content of the file\n",
    "\n",
    "Pay attention to how we define the `FileContent` class. The class name, its attributes, and the descriptions will all be passed to the LLM, as it is needed to force a specific schema for its outputs. More information [here]().\n",
    "\n",
    "The function `read_files` will receive a list of file paths and will return a list of `FileContent` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# FileContent Definition\n",
    "class FileContent(BaseModel):\n",
    "    path: str = Field(\n",
    "        description=\"Full path to the file\"\n",
    "    )\n",
    "    content: str = Field(\n",
    "        description=\"Raw content of the file\"\n",
    "    )\n",
    "\n",
    "async def read_file_async(path: str) -> FileContent:\n",
    "    \"\"\"\n",
    "    Asynchronously loads a PDF file using LangChain's PyPDFLoader and extracts text content.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The file path to process.\n",
    "\n",
    "    Returns:\n",
    "        FileContent: An object containing the extracted text content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Warning: File not found {path}\")\n",
    "            return FileContent(path=path, content=\"\")\n",
    "        \n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = await asyncio.to_thread(loader.load)  # Run synchronous PDF loading in a thread\n",
    "        # the line below works well for small files, but I would suggest a different approach for large file\n",
    "        content = \"\\n\".join(page.page_content for page in pages)\n",
    "        return FileContent(path=path, content=content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {str(e)}\")\n",
    "        return FileContent(path=path, content=\"\")\n",
    "\n",
    "async def read_files(file_list: List[str]) -> List[FileContent]:\n",
    "    \"\"\"\n",
    "    Asynchronously loads multiple PDF files and extracts text content.\n",
    "    \n",
    "    Args:\n",
    "        file_list (List[str]): The list of file paths to process.\n",
    "\n",
    "    Returns:\n",
    "        List[FileContent]: A list of FileContent objects containing extracted text.\n",
    "    \"\"\"\n",
    "    tasks = [read_file_async(path) for path in file_list]\n",
    "    return await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load some files.\n",
    "\n",
    "I have a bunch of pdfs files in the `~/Desktop/pdfs` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.expanduser('~/Desktop/pdfs')\n",
    "\n",
    "input_files = \\\n",
    "    [\n",
    "        os.path.join(base_dir, \"1.pdf\"),\n",
    "        os.path.join(base_dir, \"2.pdf\"),\n",
    "        os.path.join(base_dir, \"3.pdf\"),\n",
    "        os.path.join(base_dir, \"4.pdf\"),\n",
    "        os.path.join(base_dir, \"5.pdf\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_files = await read_files(input_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the content of the first 3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for content in processed_files[:3]:\n",
    "    print(content.path)\n",
    "    print(content.content)\n",
    "    print(\"------\")\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM invocation\n",
    "\n",
    "Lets set aside the pdf files for a moment, while we set everything up to use an LLM.\n",
    "\n",
    "We will use it to summarize the content of the files.\n",
    "\n",
    "Before invoking the LLM, we will need to define some parameters, including defining which model we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n",
      "Parameters:\n",
      "max_tokens: 8192, temperature: 0.1, top_p: 0.4\n",
      "Using Anthropic. Model: claude-3-5-haiku-20241022\n"
     ]
    }
   ],
   "source": [
    "from utils import load_llm\n",
    "\n",
    "llm = load_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need to invoke the LLM. Let's do a quick test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use the code above to do the summarization, but as explained before, forcing the LLM output to follow a specific schema is a good idea.\n",
    "\n",
    "As a recap, our use case is to parse random pdf files from newsletters, docs, etc. Each file will contain multiple discrete sections, and the LLM will be tasked with identifying and summarizing each section.\n",
    "\n",
    "To use schema validation we can use `with_structured_output` from Langchain. More information [here](https://python.langchain.com/docs/how_to/structured_output/). To do so we will need to have a custom class to define the schema.\n",
    "\n",
    "Therefore, we will create 2 classes:\n",
    "- `SectionSchema`: will define the schema for each individual section.\n",
    "- `FileSummary`: will contain all news from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterator\n",
    "\n",
    "class Section(BaseModel):\n",
    "    title: str = Field(\n",
    "        description=\"Title of the section\",\n",
    "        min_length = 5,\n",
    "        max_length = 100\n",
    "    )\n",
    "    summary: str = Field(\n",
    "        description=\"Comprehensive summary of the file content\",\n",
    "        min_length = 100,\n",
    "        max_length = 1000\n",
    "    )\n",
    "\n",
    "class FileSummary(BaseModel):\n",
    "    path: str = Field(\n",
    "        description=\"Full path to the file\"\n",
    "    )\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"List of sections extracted from the file\",\n",
    "        min_items=1\n",
    "    )\n",
    "    category: str = Field(\n",
    "        description=\"Category for the field. It can be a newsletter, documentation page, etc.\"\n",
    "    )\n",
    "\n",
    "    # Iterator object\n",
    "    def __iter__(self) -> Iterator[Section]:\n",
    "        return iter(self.sections)\n",
    "    \n",
    "    # To allow splicing\n",
    "    def __getitem__(self, index: int) -> Section:\n",
    "        return self.sections[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(FileSummary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a prompt and do some basic input sanitization.\n",
    "\n",
    "Note: Since I have direct control on the input files I am using, I wont spend too much time with input sanitization and will mostly just use Langchain's [PromptTemplates](https://python.langchain.com/docs/concepts/prompt_templates/).\n",
    "\n",
    "We will build a simple prompt and define a placeholder for the content we want to summarize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "<text>{content}</text>\n",
    "\n",
    "You are an expert at analyzing and summarizing files.\n",
    "Analyze the text contained within the <text> tags and identify:\n",
    "- individual sections within the file\n",
    "- an overall category for the file (newsletter, documentation page, blogpost, etc)\n",
    "For each section, create a title and a summary.\n",
    "\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"content\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything we need to invoke the LLM with the specified schema. Lets try it out with a single file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_response = structured_llm.invoke(prompt.format(content=processed_files[0].content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction to LangGraph: Core Concepts\n",
      "LangGraph is a library for modeling agent workflows as graphs. It uses three key components: State (a shared data structure), Nodes (Python functions encoding agent logic), and Edges (functions determining node execution flow). The library allows creating complex, looping workflows that evolve state over time, with nodes and edges being flexible Python functions that can contain LLMs or standard Python code.\n",
      "---------------\n",
      "Graph Execution and Message Passing\n",
      "LangGraph uses a message-passing algorithm inspired by Google's Pregel system. Execution proceeds in discrete 'super-steps', where nodes become active when receiving messages. Nodes run their functions, send updates, and can run in parallel or sequentially. The graph execution terminates when all nodes are inactive and no messages are in transit.\n",
      "---------------\n",
      "StateGraph and Graph State Management\n",
      "The StateGraph class is the main graph class, parameterized by a user-defined State object. State can be defined using TypedDict or Pydantic BaseModel. Nodes communicate through a shared schema, and the graph supports private state channels, different input/output schemas, and reducer functions that control how state updates are applied to different keys.\n",
      "---------------\n",
      "Working with Messages in Graph State\n",
      "LangGraph provides robust support for managing conversation messages within graph state. It offers a prebuilt add_messages reducer function to handle message tracking, updates, and deserialization. The MessagesState provides a standard way to include message history, with support for appending and updating messages while maintaining message IDs.\n",
      "---------------\n",
      "Nodes, Edges, and Control Flow\n",
      "LangGraph supports various edge types including normal edges, conditional edges, and entry points. Nodes can be Python functions with optional configuration. The library provides advanced control flow mechanisms like Send, Command, and conditional routing, allowing dynamic graph navigation and state updates within node functions.\n",
      "---------------\n",
      "Advanced LangGraph Features\n",
      "LangGraph offers advanced features like built-in persistence through checkpointers, thread management, document storage, graph migrations, configuration options, recursion limits, interrupts for human-in-the-loop workflows, breakpoints, and support for creating subgraphs. These features enable complex agent architectures with flexibility and robust state management.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "for section in test_response:\n",
    "    print(section.title)\n",
    "    print(section.summary)\n",
    "    print(\"---------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding the logic to summarize all files\n",
    "\n",
    "Now that we have a summary of a single file, lets do the same across all files.\n",
    "\n",
    "It would be a good idea to build a function that does so.\n",
    "\n",
    "Since we already have both the `FileContent` and `FileSummary` classes defined, we can use them both.\n",
    "\n",
    "The new function will receive the list of `FileContent` we created before and return a list of `FileSummary` objects.\n",
    "\n",
    "Note: I decided to not use concurrency on this function, as I am running a local LLM and my setup is better suited to process LLM request one at a time. If your setup does, I suggest you to implement concurrency :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "\n",
    "def summarize_files(file_list: List[FileContent], llm: BaseLanguageModel) -> List[FileSummary]:\n",
    "    \"\"\"\n",
    "    Summarize PDF files using an LLM and retuns a list of\n",
    "    FileSummary objects with the summarize news content.\n",
    "    \n",
    "    Args:\n",
    "        file_list (list): the list of files to summarize\n",
    "        llm (BaseLanguageModel): an LLM to use for summarization\n",
    "\n",
    "    Returns:\n",
    "        List[FileSummary]: List of FileSummary objects with the summarized news\n",
    "    \"\"\"\n",
    "\n",
    "    structured_llm = llm.with_structured_output(FileSummary)\n",
    "\n",
    "    file_summary_list = []\n",
    "\n",
    "    for file in file_list:\n",
    "        print(f\"processing file {file.path}\")\n",
    "        try:           \n",
    "            file_summary = structured_llm.invoke(prompt.format(content=file.content))\n",
    "            file_summary.path = file.path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating title & summary {str(e)}\")\n",
    "            file_summary = FileSummary(path=file.path, news_items=[])\n",
    "\n",
    "\n",
    "        # create new FileContent object with the extracted text\n",
    "        file_summary_list.append(file_summary)\n",
    "\n",
    "    return file_summary_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_files = summarize_files(file_list=processed_files, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sections in summarized_files[0]:\n",
    "    print(f\"title: {sections.title}\\nsummary: {sections.summary}\")\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes & utils re-utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier in the following notebooks, we will save the class definitions and functions we created on this notebook into a sepparate file, `utils.py` that we can import.\n",
    "\n",
    "I'll manually copy the classes and do some small modifications for this to work as an import.\n",
    "\n",
    "Then we'll test the code works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n",
      "Parameters:\n",
      "max_tokens: 8192, temperature: 0.1, top_p: 0.4\n",
      "Using Anthropic. Model: claude-3-5-haiku-20241022\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Classes\n",
    "from utils import FileContent, Section, FileSummary\n",
    "\n",
    "# Functions\n",
    "from utils import read_files, summarize_files, load_llm\n",
    "\n",
    "llm = load_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.expanduser('~/Desktop/pdfs')\n",
    "\n",
    "input_files = \\\n",
    "    [\n",
    "        os.path.join(base_dir, \"1.pdf\"),\n",
    "        os.path.join(base_dir, \"2.pdf\"),\n",
    "        os.path.join(base_dir, \"3.pdf\"),\n",
    "        os.path.join(base_dir, \"4.pdf\"),\n",
    "        os.path.join(base_dir, \"5.pdf\")\n",
    "]\n",
    "processed_files = await read_files(input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_files = summarize_files(file_list=processed_files, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: Introduction to LangGraph\n",
      "summary: LangGraph is a library that models agent workflows as graphs. It uses three key components: State (a shared data structure), Nodes (Python functions encoding agent logic), and Edges (functions determining node execution flow). The core mechanism involves message passing between nodes in discrete 'super-steps', allowing complex, evolving workflows with flexible state management.\n",
      "--------\n",
      "title: Graph State and Reducers\n",
      "summary: The graph state is defined using TypedDict or Pydantic BaseModel. Each state key can have an independent reducer function that determines how updates are applied. By default, updates override existing values, but custom reducers can implement more complex update strategies like list concatenation. The state can include message histories, with special handling for message tracking and deserialization.\n",
      "--------\n",
      "title: Nodes and Edges\n",
      "summary: Nodes are Python functions that process the graph state, while edges define routing logic between nodes. LangGraph supports different edge types: normal edges for direct node-to-node transitions, conditional edges for dynamic routing, and special START and END nodes. Nodes can have multiple outgoing edges, with destination nodes potentially executing in parallel during a superstep.\n",
      "--------\n",
      "title: Advanced Graph Control Flow\n",
      "summary: LangGraph provides advanced control flow mechanisms like Send (for dynamic edge generation), Command (for combining state updates and routing), and support for navigating between parent and subgraphs. These features enable complex workflows like map-reduce patterns, multi-agent systems, and human-in-the-loop interactions.\n",
      "--------\n",
      "title: Persistence and Configuration\n",
      "summary: LangGraph offers built-in persistence through checkpointers, allowing state snapshots and resumption of conversations. It supports threads for individual sessions, document storage for cross-thread persistence, and configuration options for dynamically switching models or system prompts. Additional features include recursion limits, interrupts for collecting user input, and breakpoints for step-by-step execution.\n",
      "--------\n",
      "title: Subgraphs and Visualization\n",
      "summary: Subgraphs allow encapsulation of graph components, enabling modular design of complex workflows. They can be added to parent graphs either as compiled graphs or through transformation functions. LangGraph also provides visualization capabilities and first-class streaming support for graph executions.\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "for sections in summarized_files[0]:\n",
    "    print(f\"title: {sections.title}\\nsummary: {sections.summary}\")\n",
    "    print(\"--------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "building-effective-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
